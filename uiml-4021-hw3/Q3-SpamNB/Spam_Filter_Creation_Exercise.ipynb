{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d97ae4",
   "metadata": {},
   "source": [
    "\n",
    "# Spam Filter Creation using Naive Bayes\n",
    "\n",
    "In this exercise, you will create a spam filter using the Naive Bayes classifier. This exercise will help you understand how to preprocess text data, implement a Naive Bayes model, and evaluate its performance.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Complete the code in the sections marked `# TODO`. Make sure to run each cell in order to see the output of your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56fdf0",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Data Loading and Preprocessing\n",
    "\n",
    "First, we need to load and preprocess our dataset. The dataset consists of emails categorized into 'spam' and 'non-spam'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9962e47",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50cdf0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44df942",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecdaab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Dataset\\spam_ham_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070de91b",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1e939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ad245",
   "metadata": {},
   "source": [
    "Pre-Process Steps\n",
    "1. Convert String to Lower-case\n",
    "2. Remove Punctuations\n",
    "3. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6e07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    words = text.split() \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    preprocessed_text = ' '.join(words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d53447db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'] = dataset['text'].apply(preprocess_text)\n",
    "\n",
    "dataset = dataset.drop(['Unnamed: 0', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd3c1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject enron methanol meter 988291 follow not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject hpl nom january 9 2001 see attached fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject neon retreat ho ho ho around wonderful...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject indian springs deal book teco pvr reve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_num\n",
       "0  subject enron methanol meter 988291 follow not...          0\n",
       "1  subject hpl nom january 9 2001 see attached fi...          0\n",
       "2  subject neon retreat ho ho ho around wonderful...          0\n",
       "3  subject photoshop windows office cheap main tr...          1\n",
       "4  subject indian springs deal book teco pvr reve...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e2850",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Feature Extraction\n",
    "\n",
    "Now, we'll convert the text data into numerical features using techniques like TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de216ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, mode=\"tf-idf\"):\n",
    "\n",
    "    if mode == \"tf-idf\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif mode == \"bag-of-words\":\n",
    "        vectorizer = CountVectorizer()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'tf-idf' or 'bag-of-words'.\")\n",
    "    \n",
    "    features = vectorizer.fit_transform(data['text'])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e3c88e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10400081 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "mode = \"tf-idf\"\n",
    "features = extract_features(dataset, mode=mode)\n",
    "print(features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c34515",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c2a5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, dataset['label_num'].values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3ebfb",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Model Training\n",
    "\n",
    "Next, implement and train the Naive Bayes classifier using the features extracted in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eddfae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.class_priors = None\n",
    "        self.likelihoods = None\n",
    "\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        num_docs, num_words = features.shape\n",
    "        self.class_priors = np.zeros(self.num_classes)\n",
    "        self.likelihoods = np.zeros((self.num_classes, num_words))\n",
    "\n",
    "        for c in range(self.num_classes):\n",
    "            # Calculate priors\n",
    "            docs_in_class = labels == c\n",
    "            self.class_priors[c] = np.sum(docs_in_class) / num_docs\n",
    "\n",
    "            # Calculate likellihoods\n",
    "            self.likelihoods[c] = (np.sum(features[docs_in_class], axis=0) + 1) / (np.sum(features[docs_in_class]) + num_words)\n",
    "\n",
    "    \n",
    "    def predict(self, features):\n",
    "        num_docs, _ = features.shape\n",
    "        predictions = []\n",
    "\n",
    "        for doc in range(num_docs):\n",
    "            log_probs = np.zeros(self.num_classes)\n",
    "\n",
    "            for c in range(self.num_classes):\n",
    "                # Calculate log probability for each class\n",
    "                log_probs[c] = np.log(self.class_priors[c]) + np.sum(np.log(self.likelihoods[c, features[doc, :] > 0]))\n",
    "\n",
    "            # Choose the class with the highest log probability\n",
    "            predicted_class = np.argmax(log_probs)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844f459",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a05c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # ham (0), spam (1)\n",
    "nb_classifier = NaiveBayesClassifier(num_classes)\n",
    "nb_classifier.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63ab17",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b0b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb_classifier.predict(X_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855e18e",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Model Evaluation\n",
    "\n",
    "Evaluate the performance of your model. Calculate metrics like accuracy, precision, and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431fc0d",
   "metadata": {},
   "source": [
    "### Percision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28f4568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percision: 0.9954545454545455\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, predictions)\n",
    "print(f\"Percision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f66e3a",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44d99366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7474402730375427\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_test, predictions)\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba7845",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4a73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8538011695906433\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, predictions)\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c8571",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "988e1eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.927536231884058\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde4eb1",
   "metadata": {},
   "source": [
    "### Save Results into .CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8458fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'Spam': predictions})\n",
    "results_df.to_csv('Q3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57508fbb",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Improvement and Discussion\n",
    "\n",
    "Discuss potential improvements to increase the model's performance. What changes could be made in preprocessing or model tuning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304bf855",
   "metadata": {},
   "source": [
    "### 1.Feature Engineering:\n",
    "Currently, we are using TF-IDF (Term Frequency-Inverse Document Frequency) for feature extraction. We can try different feature engineering techniques and see if they improve the performance. For example, We can experiment with word embeddings (e.g., Word2Vec, GloVe) or other text representations like word counts or n-grams. \n",
    "\n",
    "### 2. Advanced Text Preprocessing\n",
    "The current preprocessing step includes converting text to lowercase, removing punctuation, and removing stopwords. You can explore more advanced techniques like lemmatization, stemming, or handling special characters and URLs specific to spam emails.\n",
    "\n",
    "### 3. Ensemble Methods\n",
    "As we recently learned in our course, instead of using a single Naive Bayes classifier, you can try ensemble methods like bagging or boosting. Ensemble methods combine multiple models to improve performance and robustness.\n",
    "\n",
    "### 4. Handling Class Imbalance\n",
    "If the dataset is imbalanced (i.e., one class has significantly more samples than the other), it can affect the model's performance. You can try techniques like oversampling the minority class, undersampling the majority class, or using more advanced methods like SMOTE (Synthetic Minority Over-sampling Technique) just like what we did in previous homework!\n",
    "\n",
    "### 5. Feature Selection\n",
    "Not all features generated from text data may be relevant for distinguishing between spam and non-spam emails. Feature selection techniques can help identify the most informative features. You can explore methods like chi-squared test, mutual information, or feature importance from tree-based models to select the most discriminative features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
